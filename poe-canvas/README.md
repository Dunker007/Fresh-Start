# Nexus Workspace

> **ðŸ¤– AI Agents:** Read [AI_PROTOCOL.md](./AI_PROTOCOL.md) before starting any work.

Desktop workspace app with local LLM integration, real filesystem access, and Google ecosystem connectivity.

## Tech Stack

- **Frontend**: Vanilla JS, Tailwind CSS, Google Material Design
- **Desktop**: Tauri (Rust backend, ~600KB vs Electron's 100MB+)
- **Database**: SQLite via better-sqlite3 (TODO)
- **LLM**: LM Studio (localhost:1234) / Ollama (localhost:11434)

## Project Structure

```
poe-canvas/
â”œâ”€â”€ src/                    # Frontend source
â”‚   â”œâ”€â”€ index.html          # Main HTML
â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â””â”€â”€ main.css        # Extracted styles
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ state.js        # Central state management
â”‚       â”œâ”€â”€ llm.js          # Local LLM integration
â”‚       â”œâ”€â”€ files.js        # Filesystem operations (TODO)
â”‚       â””â”€â”€ main.js         # App initialization (TODO)
â”œâ”€â”€ src-tauri/              # Rust backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.rs         # Tauri commands
â”‚   â”œâ”€â”€ Cargo.toml          # Rust dependencies
â”‚   â””â”€â”€ tauri.conf.json     # Tauri configuration
â”œâ”€â”€ src-original/           # Original monolithic index.html
â”œâ”€â”€ package.json            # Node dependencies
â””â”€â”€ vite.config.js          # Dev server config
```

## Quick Start

```powershell
# Install dependencies
npm install

# Run in browser (dev mode)
npm run dev

# Run as desktop app
npm run tauri:dev

# Build for production
npm run tauri:build
```

## Features

### Working (from Poe Canvas)
- âœ… Dashboard with widgets
- âœ… Task management
- âœ… Notes with color coding
- âœ… Projects tracking
- âœ… Focus timer (Pomodoro)
- âœ… Mini calendar
- âœ… Virtual file manager
- âœ… Layout planner (drag zones)
- âœ… AI chat interface
- âœ… Dark/light mode
- âœ… JSON export/import

### In Progress
- ðŸ”„ Local LLM detection (llm.js)
- ðŸ”„ Modular JS architecture

### TODO (Priority Order)
1. **Real filesystem** - Read Desktop/Documents/Downloads
2. **LM Studio/Ollama chat** - Wire up llm.js to UI
3. **SQLite persistence** - Replace localStorage
4. **File watching** - Live updates with chokidar
5. **Google OAuth** - Drive/Calendar/Gmail integration

## Local LLM Setup

The app auto-detects local LLMs:

**LM Studio** (recommended):
1. Download from lmstudio.ai
2. Load a model (e.g., qwen2.5-7b)
3. Start local server (default: localhost:1234)

**Ollama**:
1. Install from ollama.ai
2. Pull a model: `ollama pull llama3.2`
3. Ollama runs automatically on localhost:11434

## Development Notes

### For Gemini/Antigravity
The `src-original/index.html` contains the complete working app (2886 lines).
Refer to it when implementing features in the modular structure.

Key patterns:
- `AppState` object holds all state
- `render*()` functions update DOM
- `setup*Listeners()` wire up events
- Modal pattern for forms

### For Claude Desktop
This project is set up for Tauri. Key files:
- `src-tauri/src/main.rs` - Native commands
- `src-tauri/tauri.conf.json` - Permissions and window config
- `src/js/llm.js` - LLM integration ready for use

## Credits

- Original canvas app built on Poe
- Handoff package prepared with architecture docs
- Tauri scaffold by Claude Desktop
